{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGM-EmbeddingToponymInterlinking\n",
    "\n",
    "This code implements a Toponym Interlinking task. A FastText embedding model is trained from scratch in order to be able to efficiently produce toponym dense representations (embeddings) which are then utilized in a binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from string import punctuation, ascii_lowercase\n",
    "from gensim.models import FastText\n",
    "from gensim.test.utils import common_texts\n",
    "import time\n",
    "from gensim.test.utils import get_tmpfile\n",
    "import pandas as pd \n",
    "from numpy.linalg import norm\n",
    "import gc\n",
    "from text_unidecode import unidecode\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1999994, 3), (499999, 3), (2499991, 3))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "val_df = pd.read_csv('./data/val.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "\n",
    "train_df.shape, val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Karpova</td>\n",
       "      <td>Карпове-Кріпенське</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dojeongyo</td>\n",
       "      <td>도전교</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gaomo</td>\n",
       "      <td>gao mo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bieddjujavri</td>\n",
       "      <td>Gåldinjavri davit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zanjon La Noria</td>\n",
       "      <td>Sitio Arqueológico La Muralla</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                s1                             s2  label\n",
       "0          Karpova             Карпове-Кріпенське      0\n",
       "1        dojeongyo                            도전교      1\n",
       "2            Gaomo                         gao mo      1\n",
       "3     Bieddjujavri              Gåldinjavri davit      0\n",
       "4  Zanjon La Noria  Sitio Arqueológico La Muralla      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def ascii_transliteration_and_punctuation_strip(s):\n",
    "    # NFKD: first applies a canonical decomposition, i.e., translates each character into its decomposed form.\n",
    "    # and afterwards apply the compatibility decomposition, i.e. replace all compatibility characters with their\n",
    "    # equivalents.\n",
    "    s = unidecode(strip_accents(s.lower()))\n",
    "    s = punctuation_regex.sub('', s)\n",
    "    return s\n",
    "\n",
    "def data_preprocessing(source):\n",
    "    source = source.replace('[^A-Za-z]',' ')\n",
    "    source = source.lower()\n",
    "    source = source.replace(\"\\s\\s+\" , \" \")\n",
    "    source = source.replace('\\s+[a-z]{1,2}(?!\\S)',' ')\n",
    "    source = source.replace(\"\\s\\s+\" , \" \")\n",
    "    return source\n",
    "\n",
    "punctuation_regex = re.compile(u'[‘’“”\\'\"!?;/⧸⁄‹›«»`ʿ,.-]')\n",
    "\n",
    "\n",
    "train_df.iloc[:,0]=train_df.iloc[:,0].apply(lambda row: ascii_transliteration_and_punctuation_strip(row))\n",
    "train_df.iloc[:,1]=train_df.iloc[:,1].apply(lambda row: ascii_transliteration_and_punctuation_strip(row))\n",
    "val_df.iloc[:,0]=val_df.iloc[:,0].apply(lambda row: ascii_transliteration_and_punctuation_strip(row))\n",
    "val_df.iloc[:,1]=val_df.iloc[:,1].apply(lambda row: ascii_transliteration_and_punctuation_strip(row))\n",
    "test_df.iloc[:,0]=test_df.iloc[:,0].apply(lambda row: ascii_transliteration_and_punctuation_strip(row))\n",
    "test_df.iloc[:,1]=test_df.iloc[:,1].apply(lambda row: ascii_transliteration_and_punctuation_strip(row))\n",
    "\n",
    "train_df.iloc[:,1] = train_df.iloc[:,1].apply(lambda row: data_preprocessing(row))\n",
    "train_df.iloc[:,0] = train_df.iloc[:,0].apply(lambda row: data_preprocessing(row))\n",
    "val_df.iloc[:,1] = val_df.iloc[:,1].apply(lambda row: data_preprocessing(row))\n",
    "val_df.iloc[:,0] = val_df.iloc[:,0].apply(lambda row: data_preprocessing(row))\n",
    "test_df.iloc[:,1] = test_df.iloc[:,1].apply(lambda row: data_preprocessing(row))\n",
    "test_df.iloc[:,0] = test_df.iloc[:,0].apply(lambda row: data_preprocessing(row))\n",
    "\n",
    "train_df.iloc[:,1] = train_df.iloc[:,1].apply(lambda x: x.translate(str.maketrans('','','1234567890')))\n",
    "train_df.iloc[:,0] = train_df.iloc[:,0].apply(lambda x: x.translate(str.maketrans('','','1234567890')))\n",
    "val_df.iloc[:,1] = val_df.iloc[:,1].apply(lambda x: x.translate(str.maketrans('','','1234567890')))\n",
    "val_df.iloc[:,0] = val_df.iloc[:,0].apply(lambda x: x.translate(str.maketrans('','','1234567890')))\n",
    "test_df.iloc[:,1] = test_df.iloc[:,1].apply(lambda x: x.translate(str.maketrans('','','1234567890')))\n",
    "test_df.iloc[:,0] = test_df.iloc[:,0].apply(lambda x: x.translate(str.maketrans('','','1234567890')))\n",
    "\n",
    "train_df.iloc[:,1] = train_df.iloc[:,1].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "train_df.iloc[:,0] = train_df.iloc[:,0].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "val_df.iloc[:,1] = val_df.iloc[:,1].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "val_df.iloc[:,0] = val_df.iloc[:,0].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "test_df.iloc[:,1] = test_df.iloc[:,1].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "test_df.iloc[:,0] = test_df.iloc[:,0].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "train_df.iloc[:,1]=train_df.iloc[:,1].apply(lambda x: x.lower())\n",
    "train_df.iloc[:,0]=train_df.iloc[:,0].apply(lambda x: x.lower())\n",
    "val_df.iloc[:,1]=val_df.iloc[:,1].apply(lambda x: x.lower())\n",
    "val_df.iloc[:,0]=val_df.iloc[:,0].apply(lambda x: x.lower())\n",
    "test_df.iloc[:,1]=test_df.iloc[:,1].apply(lambda x: x.lower())\n",
    "test_df.iloc[:,0]=test_df.iloc[:,0].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>karpova</td>\n",
       "      <td>karpovekripenske</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dojeongyo</td>\n",
       "      <td>dojeongyo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gaomo</td>\n",
       "      <td>gao mo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bieddjujavri</td>\n",
       "      <td>galdinjavri davit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zanjon la noria</td>\n",
       "      <td>sitio arqueologico la muralla</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                s1                             s2  label\n",
       "0          karpova               karpovekripenske      0\n",
       "1        dojeongyo                      dojeongyo      1\n",
       "2            gaomo                         gao mo      1\n",
       "3     bieddjujavri              galdinjavri davit      0\n",
       "4  zanjon la noria  sitio arqueologico la muralla      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation for embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999767/999767 [00:21<00:00, 45938.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# For each matching toponym pair <T1, T2>:\n",
    "#   we split T1, T2 into their tokens T1t1,...,T1tn and T2t1,...,T2tn\n",
    "#   we add to the training sequences list the following lists:\n",
    "#   [T1t1,...,T1tn,T2t1,...,T2tn] and [T2t1,...,T2tn,T1t1,...,T1tn]\n",
    "\n",
    "k=[]\n",
    "for m in tqdm(train_df[train_df['label']==1].index):\n",
    "    k.append((train_df.iloc[m,0] + \" \" + train_df.iloc[m,1]).split())\n",
    "    k.append((train_df.iloc[m,1] + \" \" + train_df.iloc[m,0]).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the FastText model\n",
    "model = FastText(size=100, window=3, min_count=1)\n",
    "model.build_vocab(sentences=k)\n",
    "model.train(sentences=k, total_examples=len(k), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname = \"fasttext.model\"\n",
    "# model.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname = \"fasttext.model\"\n",
    "# model = FastText.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1999994/1999994 [02:55<00:00, 11364.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# Here we built the feautures as the concatenation\n",
    "# of the embeddings of the pair's toponyms\n",
    "\n",
    "train_list=[]\n",
    "for i in tqdm(train_df.index):\n",
    "    train_list.append(np.concatenate((model.wv[train_df.iloc[i,0]], model.wv[train_df.iloc[i,1]]),axis=0))\n",
    "\n",
    "X_train=np.array(train_list)\n",
    "X_train=np.reshape(X_train,(X_train.shape[0],X_train.shape[1]))\n",
    "\n",
    "y_train=np.array(train_df.iloc[:,2])\n",
    "y_train=np.reshape(y_train,(y_train.shape[0]))\n",
    "y_train=y_train.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 499999/499999 [00:49<00:00, 10186.83it/s]\n"
     ]
    }
   ],
   "source": [
    "val_list=[]\n",
    "for i in tqdm(val_df.index):\n",
    "    val_list.append(np.concatenate((model.wv[val_df.iloc[i,0]], model.wv[val_df.iloc[i,1]]),axis=0))\n",
    "\n",
    "X_val=np.array(val_list)\n",
    "X_val=np.reshape(X_val,(X_val.shape[0],X_val.shape[1]))\n",
    "\n",
    "y_val=np.array(val_df.iloc[:,2])\n",
    "y_val=np.reshape(y_val,(y_val.shape[0]))\n",
    "y_val=y_val.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2499991/2499991 [04:04<00:00, 10213.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list=[]\n",
    "for i in tqdm(test_df.index):\n",
    "    test_list.append(np.concatenate((model.wv[test_df.iloc[i,0]],model.wv[test_df.iloc[i,1]]),axis=0))\n",
    "\n",
    "X_test = np.array(test_list)\n",
    "X_test=np.reshape(X_test,(X_test.shape[0],X_test.shape[1]))\n",
    "\n",
    "y_test=np.array(test_df.iloc[:, 2])\n",
    "y_test=np.reshape(y_test,(y_test.shape[0]))\n",
    "y_test=y_test.astype('int')\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification through a fully-connected NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes (lyricists): 1\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 2048)              411648    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,461,249\n",
      "Trainable params: 1,461,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "nb_classes = 1\n",
    "max_words=200\n",
    "print('Number of Classes: {}'.format(nb_classes))\n",
    "\n",
    "# Number of Epochs that we will train our Feed Forward Network\n",
    "nb_epoch = 30\n",
    "# The batch_size of the data that  will be fed to the Model when training\n",
    "batch_size = 1024 \n",
    "# Dropout Rate of the Dropout Layer\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(2048, input_shape=(max_words,)))\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Dropout(dropout_rate))\n",
    "model1.add(Dense(512))\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Dropout(dropout_rate))\n",
    "model1.add(Dense(nb_classes))\n",
    "model1.add(Activation('sigmoid'))\n",
    "\n",
    "# print model layers' info\n",
    "print(model1.summary())\n",
    "\n",
    "\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "model1.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=[\n",
    "        'accuracy', f1_m, precision_m,recall_m\n",
    "    ],\n",
    ")\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1999994 samples, validate on 499999 samples\n",
      "Epoch 1/30\n",
      "1999994/1999994 [==============================] - 8s 4us/sample - loss: 0.4465 - accuracy: 0.7924 - f1_m: 0.7869 - precision_m: 0.8025 - recall_m: 0.7765 - val_loss: 0.3933 - val_accuracy: 0.8239 - val_f1_m: 0.8229 - val_precision_m: 0.8279 - val_recall_m: 0.8182\n",
      "Epoch 2/30\n",
      "1999994/1999994 [==============================] - 7s 3us/sample - loss: 0.3881 - accuracy: 0.8274 - f1_m: 0.8252 - precision_m: 0.8351 - recall_m: 0.8161 - val_loss: 0.3785 - val_accuracy: 0.8323 - val_f1_m: 0.8313 - val_precision_m: 0.8362 - val_recall_m: 0.8268\n",
      "Epoch 3/30\n",
      "1999994/1999994 [==============================] - 7s 3us/sample - loss: 0.3724 - accuracy: 0.8355 - f1_m: 0.8341 - precision_m: 0.8409 - recall_m: 0.8279 - val_loss: 0.3690 - val_accuracy: 0.8376 - val_f1_m: 0.8372 - val_precision_m: 0.8395 - val_recall_m: 0.8353\n",
      "Epoch 4/30\n",
      "1999994/1999994 [==============================] - 7s 3us/sample - loss: 0.3620 - accuracy: 0.8409 - f1_m: 0.8399 - precision_m: 0.8447 - recall_m: 0.8357 - val_loss: 0.3679 - val_accuracy: 0.8382 - val_f1_m: 0.8372 - val_precision_m: 0.8427 - val_recall_m: 0.8321\n",
      "Epoch 5/30\n",
      "1999994/1999994 [==============================] - 7s 3us/sample - loss: 0.3534 - accuracy: 0.8454 - f1_m: 0.8447 - precision_m: 0.8481 - recall_m: 0.8417 - val_loss: 0.3607 - val_accuracy: 0.8429 - val_f1_m: 0.8424 - val_precision_m: 0.8455 - val_recall_m: 0.8395\n",
      "Epoch 6/30\n",
      "1999994/1999994 [==============================] - 7s 3us/sample - loss: 0.3460 - accuracy: 0.8489 - f1_m: 0.8483 - precision_m: 0.8509 - recall_m: 0.8461 - val_loss: 0.3592 - val_accuracy: 0.8443 - val_f1_m: 0.8438 - val_precision_m: 0.8466 - val_recall_m: 0.8412\n",
      "Epoch 7/30\n",
      "1999994/1999994 [==============================] - 7s 3us/sample - loss: 0.3397 - accuracy: 0.8518 - f1_m: 0.8514 - precision_m: 0.8531 - recall_m: 0.8501 - val_loss: 0.3557 - val_accuracy: 0.8450 - val_f1_m: 0.8443 - val_precision_m: 0.8486 - val_recall_m: 0.8403\n",
      "Epoch 8/30\n",
      "1999994/1999994 [==============================] - 7s 3us/sample - loss: 0.3339 - accuracy: 0.8547 - f1_m: 0.8544 - precision_m: 0.8556 - recall_m: 0.8535 - val_loss: 0.3567 - val_accuracy: 0.8446 - val_f1_m: 0.8463 - val_precision_m: 0.8371 - val_recall_m: 0.8559\n",
      "Epoch 9/30\n",
      "1999994/1999994 [==============================] - 7s 3us/sample - loss: 0.3285 - accuracy: 0.8572 - f1_m: 0.8570 - precision_m: 0.8574 - recall_m: 0.8569 - val_loss: 0.3548 - val_accuracy: 0.8463 - val_f1_m: 0.8458 - val_precision_m: 0.8488 - val_recall_m: 0.8430\n",
      "Epoch 10/30\n",
      "1999994/1999994 [==============================] - 7s 3us/sample - loss: 0.3234 - accuracy: 0.8594 - f1_m: 0.8594 - precision_m: 0.8590 - recall_m: 0.8601 - val_loss: 0.3528 - val_accuracy: 0.8468 - val_f1_m: 0.8465 - val_precision_m: 0.8483 - val_recall_m: 0.8450\n",
      "Epoch 11/30\n",
      "1999994/1999994 [==============================] - 7s 3us/sample - loss: 0.3188 - accuracy: 0.8614 - f1_m: 0.8614 - precision_m: 0.8606 - recall_m: 0.8625 - val_loss: 0.3533 - val_accuracy: 0.8475 - val_f1_m: 0.8476 - val_precision_m: 0.8474 - val_recall_m: 0.8480\n",
      "Epoch 12/30\n",
      "1999994/1999994 [==============================] - 7s 3us/sample - loss: 0.3140 - accuracy: 0.8636 - f1_m: 0.8636 - precision_m: 0.8624 - recall_m: 0.8651 - val_loss: 0.3529 - val_accuracy: 0.8472 - val_f1_m: 0.8485 - val_precision_m: 0.8420 - val_recall_m: 0.8553\n",
      "Epoch 13/30\n",
      "1999994/1999994 [==============================] - 7s 3us/sample - loss: 0.3099 - accuracy: 0.8656 - f1_m: 0.8658 - precision_m: 0.8640 - recall_m: 0.8679 - val_loss: 0.3550 - val_accuracy: 0.8477 - val_f1_m: 0.8471 - val_precision_m: 0.8506 - val_recall_m: 0.8439\n"
     ]
    }
   ],
   "source": [
    "# We train (fit our data to) our model\n",
    "history = model1.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=nb_epoch,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=1,\n",
    "    callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2499991/2499991 [==============================] - 4s 2us/sample - loss: 0.3542 - accuracy: 0.8481 - f1_m: 0.8474 - precision_m: 0.8511 - recall_m: 0.8439\n"
     ]
    }
   ],
   "source": [
    "# evaluate and store on score variable on the TEST DATASET\n",
    "score = model1.evaluate(\n",
    "    X_test,\n",
    "    y_test,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
